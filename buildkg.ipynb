{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity(BaseModel):\n",
    "    \"\"\"Represents an entity in the Knowledge Graph.\"\"\"\n",
    "    id: str = Field(description=\"Unique identifier for the entity in Vietnamese.\")\n",
    "    label: List[str] = Field(description=\"Array of entity names/aliases in Vietnamese.\")\n",
    "    type: str = Field(description=\"Entity type in Vietnamese.\")\n",
    "    description: str = Field(description=\"Brief description of the entity in Vietnamese.\")\n",
    "    original_text: str = Field(description=\"Complete verbatim quote from source text related to this entity.\")\n",
    "    properties: Dict[str, Any] = Field(description=\"Object containing important structured information.\")\n",
    "    metadata: Dict[str, Any] = Field(description=\"Object containing all additional available information.\")\n",
    "\n",
    "\n",
    "class Triplet(BaseModel):\n",
    "    \"\"\"Represents a relationship (triplet) in the Knowledge Graph.\"\"\"\n",
    "    subject_id: str = Field(description=\"Vietnamese ID of the subject entity.\")\n",
    "    predicate: str = Field(description=\"Relationship name, a concise phrase/verb in Vietnamese.\")\n",
    "    object_id: str = Field(description=\"Vietnamese ID of the object entity.\")\n",
    "    properties: Optional[Dict[str, Any]] = Field(default=None, description=\"Structured information about the relationship.\")\n",
    "    metadata: Optional[Dict[str, Any]] = Field(default=None, description=\"Additional information for the relationship.\")\n",
    "\n",
    "\n",
    "class KnowledgeGraph(BaseModel):\n",
    "    \"\"\"Complete structure of Knowledge Graph containing entities and relationships.\"\"\"\n",
    "    entities: List[Entity] = Field(description=\"List of all extracted entities.\")\n",
    "    triplets: List[Triplet] = Field(description=\"List of all extracted relationships.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_PROMPT = \"\"\"You are an expert at extracting Knowledge Graphs from Vietnamese historical texts. Your task is to extract ALL entities and relationships.\n",
    "\n",
    "## OUTPUT FORMAT:\n",
    "Return ONLY a valid JSON object (no markdown, no explanations):\n",
    "\n",
    "{{\n",
    "  \"entities\": [\n",
    "    {{\n",
    "      \"id\": \"entity_id_in_vietnamese\",\n",
    "      \"label\": [\"array of Vietnamese names/aliases\"],\n",
    "      \"type\": \"Nhân vật|Tổ chức|Sự kiện|Hiệp ước|Địa điểm|Quốc gia|Khái niệm|Khác\",\n",
    "      \"description\": \"Brief Vietnamese description\",\n",
    "      \"original_text\": \"COMPLETE verbatim quote from source - this is CRITICAL for quiz answering\",\n",
    "      \"properties\": {{\"structured_data\": \"value\"}},\n",
    "      \"metadata\": {{\"additional_info\": \"value\"}}\n",
    "    }}\n",
    "  ],\n",
    "  \"triplets\": [\n",
    "    {{\n",
    "      \"subject_id\": \"must_match_entity_id\",\n",
    "      \"predicate\": \"vietnamese_verb_phrase\",\n",
    "      \"object_id\": \"must_match_entity_id\",\n",
    "      \"properties\": {{\"optional_structured_data\": \"value\"}},\n",
    "      \"metadata\": {{\"optional_additional_info\": \"value\"}}\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "## CRITICAL RULES:\n",
    "\n",
    "### 1. ENTITY EXTRACTION:\n",
    "- Extract ALL significant entities (people, organizations, events, places, documents, concepts)\n",
    "- **original_text is MANDATORY**: Copy the COMPLETE, EXACT text from source that describes this entity\n",
    "- Include ALL relevant sentences about the entity, not just one sentence\n",
    "- Multiple mentions → combine all text segments with \"\\\\n\\\\n\" separator\n",
    "- Properties: dates (YYYY-MM-DD format), locations, key facts\n",
    "- Metadata: roles, additional context\n",
    "- TYPE MUST BE ONE OF: Nhân vật, Tổ chức, Sự kiện, Hiệp ước, Địa điểm, Quốc gia, Khái niệm, Khác\n",
    "\n",
    "### 2. RELATIONSHIP EXTRACTION (CRITICAL - YOU MUST EXTRACT MORE):\n",
    "- **Extract EVERY relationship between entities mentioned in the text**\n",
    "- If two entities are mentioned together or related, CREATE a triplet\n",
    "- Common predicates: \"là\", \"thuộc_về\", \"tham_gia\", \"ký_kết\", \"lãnh_đạo\", \"thành_lập\", \"có_ảnh_hưởng_đến\", \"liên_quan_đến\", \"hợp_tác_với\", \"phát_triển_tại\"\n",
    "- Temporal relationships: \"diễn_ra_trước\", \"dẫn_đến\", \"kết_quả_của\"\n",
    "- DO NOT create isolated entities - ensure each entity has at least one relationship\n",
    "\n",
    "### 3. CONTINUITY WITH EXISTING ENTITIES:\n",
    "**Previously extracted entities:**\n",
    "{existing_entities_summary}\n",
    "\n",
    "**When you encounter these entities again:**\n",
    "- Reference by EXACT ID\n",
    "- Add new labels if found\n",
    "- APPEND to original_text (with \\\\n\\\\n separator)\n",
    "- Create NEW relationships with other entities in current chunk\n",
    "\n",
    "### 4. RELATIONSHIP COMPLETENESS CHECK:\n",
    "Before returning, verify:\n",
    "- Every entity has AT LEAST one triplet (as subject OR object)\n",
    "- Related entities mentioned together have explicit relationships\n",
    "- Temporal sequences are captured (event A → event B)\n",
    "- Organizational hierarchies are clear (entity X belongs_to entity Y)\n",
    "\n",
    "### 5. QUALITY REQUIREMENTS:\n",
    "- All IDs in Vietnamese (lowercase, underscores for spaces)\n",
    "- All values in Vietnamese\n",
    "- Dates in YYYY-MM-DD, YYYY-MM-00, or YYYY-00-00 format\n",
    "- NO abstract concepts without concrete relationships\n",
    "- Focus on factual, verifiable relationships\n",
    "\n",
    "## PROCESSING STRATEGY:\n",
    "1. Identify all entities in the chunk\n",
    "2. For EACH entity pair that appears together or is contextually related, create a triplet\n",
    "3. Check against existing entities - update if duplicate, create relationship if new\n",
    "4. Extract original_text carefully - include ALL relevant sentences\n",
    "5. Verify completeness before returning\n",
    "\n",
    "Now process the following text:\n",
    "\n",
    "{document_content}\"\"\"\n",
    "\n",
    "# Simplified prompt for retry attempts\n",
    "SIMPLE_EXTRACTION_PROMPT = \"\"\"Extract entities and relationships from this Vietnamese text. Return ONLY valid JSON:\n",
    "\n",
    "{{\n",
    "  \"entities\": [\n",
    "    {{\n",
    "      \"id\": \"vietnamese_id\",\n",
    "      \"label\": [\"names\"],\n",
    "      \"type\": \"Nhân vật|Tổ chức|Sự kiện|Hiệp ước|Địa điểm|Quốc gia|Khái niệm|Khác\",\n",
    "      \"description\": \"description\",\n",
    "      \"original_text\": \"exact text from source\",\n",
    "      \"properties\": {{}},\n",
    "      \"metadata\": {{}}\n",
    "    }}\n",
    "  ],\n",
    "  \"triplets\": [\n",
    "    {{\n",
    "      \"subject_id\": \"id1\",\n",
    "      \"predicate\": \"relationship\",\n",
    "      \"object_id\": \"id2\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Text: {document_content}\n",
    "\n",
    "Extract at least 3-5 main entities and their relationships.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(text: str, max_chunk_size: int = 1200) -> List[str]:\n",
    "    \"\"\"Split text into smaller chunks for processing.\"\"\"\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        paragraph = paragraph.strip()\n",
    "        if not paragraph:\n",
    "            continue\n",
    "            \n",
    "        if len(paragraph) > max_chunk_size:\n",
    "            sentences = re.split(r'[.!?;]+', paragraph)\n",
    "            for sentence in sentences:\n",
    "                sentence = sentence.strip()\n",
    "                if not sentence:\n",
    "                    continue\n",
    "                    \n",
    "                if len(current_chunk) + len(sentence) + 2 > max_chunk_size:\n",
    "                    if current_chunk:\n",
    "                        chunks.append(current_chunk.strip())\n",
    "                    current_chunk = sentence + \". \"\n",
    "                else:\n",
    "                    current_chunk += sentence + \". \"\n",
    "        elif len(current_chunk) + len(paragraph) + 2 > max_chunk_size:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = paragraph + \"\\n\\n\"\n",
    "        else:\n",
    "            current_chunk += paragraph + \"\\n\\n\"\n",
    "    \n",
    "    if current_chunk.strip():\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def format_existing_entities_summary(entities: List[Dict]) -> str:\n",
    "    \"\"\"Format existing entities for context.\"\"\"\n",
    "    if not entities:\n",
    "        return \"None (first chunk)\"\n",
    "    \n",
    "    summary = []\n",
    "    for entity in entities[-100:]:  # Last 100 entities for context\n",
    "        summary.append(f\"- {entity['id']} ({entity['type']}): {', '.join(entity['label'][:3])}\")\n",
    "    \n",
    "    return \"\\n\".join(summary)\n",
    "\n",
    "\n",
    "def merge_entity_data(existing: Dict, new: Dict) -> Dict:\n",
    "    \"\"\"Intelligently merge new entity data into existing entity.\"\"\"\n",
    "    # Merge labels\n",
    "    existing_labels = set(existing.get('label', []))\n",
    "    new_labels = set(new.get('label', []))\n",
    "    existing['label'] = list(existing_labels | new_labels)\n",
    "    \n",
    "    # Append original_text with separator\n",
    "    new_text = new.get('original_text', '').strip()\n",
    "    existing_text = existing.get('original_text', '').strip()\n",
    "    \n",
    "    if new_text and new_text not in existing_text:\n",
    "        if existing_text:\n",
    "            existing['original_text'] = existing_text + \"\\n\\n\" + new_text\n",
    "        else:\n",
    "            existing['original_text'] = new_text\n",
    "    \n",
    "    # Merge properties (new takes precedence)\n",
    "    existing['properties'].update(new.get('properties', {}))\n",
    "    \n",
    "    # Merge metadata\n",
    "    existing['metadata'].update(new.get('metadata', {}))\n",
    "    \n",
    "    # Use longer description\n",
    "    if len(new.get('description', '')) > len(existing.get('description', '')):\n",
    "        existing['description'] = new['description']\n",
    "    \n",
    "    return existing\n",
    "\n",
    "\n",
    "def merge_knowledge_graphs(kg_list: List[Dict]) -> Dict:\n",
    "    \"\"\"Merge multiple knowledge graphs with deduplication.\"\"\"\n",
    "    merged_entities = {}\n",
    "    merged_triplets = []\n",
    "    triplet_set = set()\n",
    "    \n",
    "    for kg in kg_list:\n",
    "        # Merge entities\n",
    "        for entity in kg.get('entities', []):\n",
    "            entity_id = entity['id']\n",
    "            \n",
    "            if entity_id in merged_entities:\n",
    "                merged_entities[entity_id] = merge_entity_data(merged_entities[entity_id], entity)\n",
    "            else:\n",
    "                merged_entities[entity_id] = entity\n",
    "        \n",
    "        # Merge triplets (deduplicate)\n",
    "        for triplet in kg.get('triplets', []):\n",
    "            triplet_key = (triplet['subject_id'], triplet['predicate'], triplet['object_id'])\n",
    "            \n",
    "            if triplet_key not in triplet_set:\n",
    "                triplet_set.add(triplet_key)\n",
    "                merged_triplets.append(triplet)\n",
    "    \n",
    "    return {\n",
    "        'entities': list(merged_entities.values()),\n",
    "        'triplets': merged_triplets\n",
    "    }\n",
    "\n",
    "\n",
    "def fix_common_json_errors(text: str) -> str:\n",
    "    \"\"\"Fix common JSON syntax errors.\"\"\"\n",
    "    # Remove trailing commas before closing brackets/braces\n",
    "    text = re.sub(r',\\s*}', '}', text)\n",
    "    text = re.sub(r',\\s*]', ']', text)\n",
    "    \n",
    "    # Fix unescaped quotes in strings (basic fix)\n",
    "    # This is tricky - only do simple cases\n",
    "    \n",
    "    # Ensure proper closing\n",
    "    open_braces = text.count('{')\n",
    "    close_braces = text.count('}')\n",
    "    if open_braces > close_braces:\n",
    "        text += '}' * (open_braces - close_braces)\n",
    "    \n",
    "    open_brackets = text.count('[')\n",
    "    close_brackets = text.count(']')\n",
    "    if open_brackets > close_brackets:\n",
    "        text += ']' * (open_brackets - close_brackets)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_response(response_text: str, fix_errors: bool = True) -> Dict:\n",
    "    \"\"\"Parse JSON from LLM response with robust error handling.\"\"\"\n",
    "    if not response_text or not response_text.strip():\n",
    "        return {\"entities\": [], \"triplets\": []}\n",
    "    \n",
    "    text = response_text.strip()\n",
    "    \n",
    "    # Remove markdown\n",
    "    text = re.sub(r'^```json\\s*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^```\\s*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\s*```$', '', text, flags=re.MULTILINE)\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Extract JSON object\n",
    "    json_start = text.find('{')\n",
    "    json_end = text.rfind('}')\n",
    "    \n",
    "    if json_start == -1 or json_end == -1:\n",
    "        return {\"entities\": [], \"triplets\": []}\n",
    "    \n",
    "    text = text[json_start:json_end+1]\n",
    "    \n",
    "    if fix_errors:\n",
    "        text = fix_common_json_errors(text)\n",
    "    \n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        # Try to extract partial data\n",
    "        try:\n",
    "            # Try to find entities array\n",
    "            entities_match = re.search(r'\"entities\"\\s*:\\s*\\[(.*?)\\]', text, re.DOTALL)\n",
    "            triplets_match = re.search(r'\"triplets\"\\s*:\\s*\\[(.*?)\\]', text, re.DOTALL)\n",
    "            \n",
    "            partial = {\"entities\": [], \"triplets\": []}\n",
    "            \n",
    "            if entities_match:\n",
    "                try:\n",
    "                    partial[\"entities\"] = json.loads('[' + entities_match.group(1) + ']')\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if triplets_match:\n",
    "                try:\n",
    "                    partial[\"triplets\"] = json.loads('[' + triplets_match.group(1) + ']')\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if partial[\"entities\"]:\n",
    "                return partial\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return {\"entities\": [], \"triplets\": []}\n",
    "\n",
    "\n",
    "def validate_and_fix_kg(kg_dict: Dict) -> Optional[Dict]:\n",
    "    \"\"\"Validate KG structure and fix common issues.\"\"\"\n",
    "    try:\n",
    "        # Fix missing required fields\n",
    "        for entity in kg_dict.get('entities', []):\n",
    "            if 'properties' not in entity:\n",
    "                entity['properties'] = {}\n",
    "            if 'metadata' not in entity:\n",
    "                entity['metadata'] = {}\n",
    "            if not isinstance(entity.get('label'), list):\n",
    "                entity['label'] = [str(entity.get('label', entity.get('id', 'unknown')))]\n",
    "            if not entity.get('original_text'):\n",
    "                entity['original_text'] = entity.get('description', '')\n",
    "        \n",
    "        for triplet in kg_dict.get('triplets', []):\n",
    "            if 'properties' not in triplet:\n",
    "                triplet['properties'] = {}\n",
    "            if 'metadata' not in triplet:\n",
    "                triplet['metadata'] = {}\n",
    "        \n",
    "        # Validate with Pydantic\n",
    "        kg_validated = KnowledgeGraph(**kg_dict)\n",
    "        return kg_validated.model_dump()\n",
    "    \n",
    "    except ValidationError as e:\n",
    "        print(f\"⚠ Validation error: {str(e)[:100]}\")\n",
    "        # Try to salvage what we can\n",
    "        try:\n",
    "            fixed_entities = []\n",
    "            for entity in kg_dict.get('entities', []):\n",
    "                try:\n",
    "                    # Ensure minimum required fields\n",
    "                    if entity.get('id') and entity.get('type') and entity.get('description'):\n",
    "                        if 'label' not in entity or not isinstance(entity['label'], list):\n",
    "                            entity['label'] = [entity['id']]\n",
    "                        if 'original_text' not in entity:\n",
    "                            entity['original_text'] = entity.get('description', '')\n",
    "                        if 'properties' not in entity:\n",
    "                            entity['properties'] = {}\n",
    "                        if 'metadata' not in entity:\n",
    "                            entity['metadata'] = {}\n",
    "                        fixed_entities.append(entity)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            fixed_triplets = []\n",
    "            for triplet in kg_dict.get('triplets', []):\n",
    "                try:\n",
    "                    if triplet.get('subject_id') and triplet.get('predicate') and triplet.get('object_id'):\n",
    "                        if 'properties' not in triplet:\n",
    "                            triplet['properties'] = {}\n",
    "                        if 'metadata' not in triplet:\n",
    "                            triplet['metadata'] = {}\n",
    "                        fixed_triplets.append(triplet)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if fixed_entities:\n",
    "                fixed_kg = {\"entities\": fixed_entities, \"triplets\": fixed_triplets}\n",
    "                kg_validated = KnowledgeGraph(**fixed_kg)\n",
    "                return kg_validated.model_dump()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def load_existing_kg(file_path: str) -> Dict:\n",
    "    \"\"\"Load existing knowledge graph from JSON file.\"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Warning: Could not load existing KG: {e}\")\n",
    "    return {\"entities\": [], \"triplets\": []}\n",
    "\n",
    "\n",
    "def save_kg(kg_dict: Dict, output_path: str):\n",
    "    \"\"\"Save knowledge graph to JSON file.\"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(kg_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "def process_chunk_with_retry(\n",
    "    chunk: str,\n",
    "    existing_summary: str,\n",
    "    llm,\n",
    "    max_retries: int = 5,\n",
    "    delay: float = 2.0\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"Process a single chunk with multiple retry strategies.\"\"\"\n",
    "    \n",
    "    strategies = [\n",
    "        (\"full\", SCHEMA_PROMPT, False),\n",
    "        (\"full_fixed\", SCHEMA_PROMPT, True),\n",
    "        (\"simple\", SIMPLE_EXTRACTION_PROMPT, False),\n",
    "        (\"simple_fixed\", SIMPLE_EXTRACTION_PROMPT, True),\n",
    "        (\"minimal\", SIMPLE_EXTRACTION_PROMPT, True)\n",
    "    ]\n",
    "    \n",
    "    for strategy_name, prompt_template, fix_json in strategies:\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "                chain = prompt | llm\n",
    "                \n",
    "                response = chain.invoke({\n",
    "                    \"document_content\": chunk,\n",
    "                    \"existing_entities_summary\": existing_summary if \"existing_entities_summary\" in prompt_template else \"\"\n",
    "                })\n",
    "                \n",
    "                if not hasattr(response, 'content') or not response.content:\n",
    "                    time.sleep(delay)\n",
    "                    continue\n",
    "                \n",
    "                kg_dict = parse_json_response(response.content, fix_errors=fix_json)\n",
    "                \n",
    "                if not kg_dict.get('entities'):\n",
    "                    time.sleep(delay)\n",
    "                    continue\n",
    "                \n",
    "                # Validate and fix\n",
    "                kg_validated = validate_and_fix_kg(kg_dict)\n",
    "                \n",
    "                if kg_validated and kg_validated.get('entities'):\n",
    "                    return kg_validated\n",
    "                \n",
    "                time.sleep(delay)\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = str(e)\n",
    "                \n",
    "                # Handle rate limit\n",
    "                if \"429\" in error_msg or \"quota\" in error_msg.lower():\n",
    "                    wait_time = 10\n",
    "                    if \"retry in\" in error_msg.lower():\n",
    "                        try:\n",
    "                            wait_match = re.search(r'retry in (\\d+\\.?\\d*)', error_msg.lower())\n",
    "                            if wait_match:\n",
    "                                wait_time = float(wait_match.group(1)) + 1\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    print(f\"⏳{wait_time:.0f}s \", end=\"\", flush=True)\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                \n",
    "                # Other errors\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(delay)\n",
    "                    continue\n",
    "        \n",
    "        # If this strategy failed, try next one\n",
    "        print(f\"[{strategy_name}→\", end=\"\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def process_single_file_incremental(\n",
    "    file_path: str,\n",
    "    existing_kg: Dict,\n",
    "    llm,\n",
    "    max_retries: int = 5,\n",
    "    delay_between_chunks: float = 15.0\n",
    ") -> Dict:\n",
    "    \"\"\"Process a single file and merge with existing KG incrementally.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error reading file: {e}\")\n",
    "        return existing_kg\n",
    "    \n",
    "    print(f\"✓ Loaded {len(content)} characters\")\n",
    "    \n",
    "    chunks = split_into_chunks(content, max_chunk_size=1200)\n",
    "    print(f\"✓ Split into {len(chunks)} chunks\\n\")\n",
    "    \n",
    "    cumulative_entities = existing_kg.get('entities', [])\n",
    "    chunk_kg_list = []\n",
    "    failed_chunks = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"Chunk {i}/{len(chunks)}: \", end=\"\", flush=True)\n",
    "        \n",
    "        existing_summary = format_existing_entities_summary(cumulative_entities)\n",
    "        \n",
    "        kg_result = process_chunk_with_retry(\n",
    "            chunk,\n",
    "            existing_summary,\n",
    "            llm,\n",
    "            max_retries=max_retries,\n",
    "            delay=delay_between_chunks\n",
    "        )\n",
    "        \n",
    "        if kg_result and kg_result.get('entities'):\n",
    "            chunk_kg_list.append(kg_result)\n",
    "            cumulative_entities.extend(kg_result['entities'])\n",
    "            \n",
    "            entity_count = len(kg_result['entities'])\n",
    "            triplet_count = len(kg_result['triplets'])\n",
    "            ratio = triplet_count / entity_count if entity_count > 0 else 0\n",
    "            \n",
    "            print(f\"] ✓ {entity_count}E/{triplet_count}T (R:{ratio:.1f})\")\n",
    "        else:\n",
    "            failed_chunks.append(i)\n",
    "            print(f\"] ✗ FAILED\")\n",
    "        \n",
    "        time.sleep(delay_between_chunks)\n",
    "    \n",
    "    if failed_chunks:\n",
    "        print(f\"\\n⚠ Failed chunks: {failed_chunks} (Total: {len(failed_chunks)}/{len(chunks)})\")\n",
    "    \n",
    "    # Merge new chunks with existing KG\n",
    "    print(f\"\\n→ Merging {len(chunk_kg_list)} successful chunks with existing KG...\")\n",
    "    all_kg_list = [existing_kg] + chunk_kg_list if existing_kg.get('entities') else chunk_kg_list\n",
    "    merged_kg = merge_knowledge_graphs(all_kg_list)\n",
    "    \n",
    "    return merged_kg\n",
    "\n",
    "\n",
    "def create_knowledge_graph_incremental(\n",
    "    file_paths: List[str],\n",
    "    output_json_path: str,\n",
    "    output_ttl_path: str = None,\n",
    "    api_key_1: str = None,\n",
    "    max_retries: int = 5,\n",
    "    delay_between_chunks: float = 2.0,\n",
    "    save_after_each_file: bool = True\n",
    "):\n",
    "    \"\"\"Create unified knowledge graph by processing files incrementally.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"INCREMENTAL KNOWLEDGE GRAPH EXTRACTION SYSTEM (WITH ERROR RECOVERY)\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    if api_key_1 is None:\n",
    "        api_key_1 = os.environ.get(\"GOOGLE_API_KEY_1\")\n",
    "        if not api_key_1:\n",
    "            print(\"ERROR: Google API key not found!\")\n",
    "            return\n",
    "    \n",
    "    print(\"[1] Initializing LLM...\")\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        google_api_key=api_key_1,\n",
    "        temperature=0.0,\n",
    "        max_tokens=8000,\n",
    "    )\n",
    "    print(\"✓ LLM initialized\\n\")\n",
    "    \n",
    "    # Load existing KG if available\n",
    "    print(\"[2] Checking for existing knowledge graph...\")\n",
    "    existing_kg = load_existing_kg(output_json_path)\n",
    "    if existing_kg.get('entities'):\n",
    "        print(f\"✓ Loaded existing KG: {len(existing_kg['entities'])} entities, {len(existing_kg['triplets'])} triplets\\n\")\n",
    "    else:\n",
    "        print(\"✓ Starting fresh knowledge graph\\n\")\n",
    "    \n",
    "    # Process each file incrementally\n",
    "    current_kg = existing_kg\n",
    "    \n",
    "    for file_idx, file_path in enumerate(file_paths, 1):\n",
    "        print(f\"\\n[3.{file_idx}] Processing file {file_idx}/{len(file_paths)}\")\n",
    "        \n",
    "        current_kg = process_single_file_incremental(\n",
    "            file_path,\n",
    "            current_kg,\n",
    "            llm,\n",
    "            max_retries=max_retries,\n",
    "            delay_between_chunks=delay_between_chunks\n",
    "        )\n",
    "        \n",
    "        # Save after each file\n",
    "        if save_after_each_file:\n",
    "            save_kg(current_kg, output_json_path)\n",
    "            print(f\"✓ Progress saved: {len(current_kg['entities'])} entities, {len(current_kg['triplets'])} triplets\")\n",
    "    \n",
    "    # Final statistics\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"FINAL STATISTICS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total Entities: {len(current_kg['entities'])}\")\n",
    "    print(f\"Total Relationships: {len(current_kg['triplets'])}\")\n",
    "    \n",
    "    ratio = len(current_kg['triplets']) / len(current_kg['entities']) if current_kg['entities'] else 0\n",
    "    print(f\"Relationship/Entity Ratio: {ratio:.2f}\")\n",
    "    \n",
    "    if ratio < 1.0:\n",
    "        print(\"⚠ WARNING: Low relationship ratio - entities may be isolated\")\n",
    "    \n",
    "    # Entity type distribution\n",
    "    type_dist = {}\n",
    "    for entity in current_kg['entities']:\n",
    "        entity_type = entity.get('type', 'Unknown')\n",
    "        type_dist[entity_type] = type_dist.get(entity_type, 0) + 1\n",
    "    \n",
    "    print(f\"\\nEntity Type Distribution:\")\n",
    "    for entity_type, count in sorted(type_dist.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {entity_type}: {count}\")\n",
    "    \n",
    "    # Final save\n",
    "    print(f\"\\n[4] Saving final outputs...\")\n",
    "    save_kg(current_kg, output_json_path)\n",
    "    print(f\"✓ JSON saved: {output_json_path}\")\n",
    "    \n",
    "    if output_ttl_path:\n",
    "        export_to_ttl(current_kg, output_ttl_path)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"EXTRACTION COMPLETE!\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return current_kg\n",
    "\n",
    "\n",
    "def export_to_ttl(kg_dict: Dict, output_path: str):\n",
    "    \"\"\"Export KG to TTL format.\"\"\"\n",
    "    ttl = \"@prefix : <http://example.org/kg#> .\\n\\n\"\n",
    "    \n",
    "    for entity in kg_dict['entities']:\n",
    "        eid = entity['id'].replace(' ', '_')\n",
    "        ttl += f\":{eid} a :{entity['type']} ;\\n\"\n",
    "        ttl += f'  :label \"{entity[\"id\"]}\"@vi ;\\n'\n",
    "        ttl += f'  :description \"{entity[\"description\"]}\"@vi .\\n\\n'\n",
    "    \n",
    "    for triplet in kg_dict['triplets']:\n",
    "        subj = triplet['subject_id'].replace(' ', '_')\n",
    "        pred = triplet['predicate'].replace(' ', '_')\n",
    "        obj = triplet['object_id'].replace(' ', '_')\n",
    "        ttl += f\":{subj} :{pred} :{obj} .\\n\"\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(ttl)\n",
    "    print(f\"✓ TTL saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "INCREMENTAL KNOWLEDGE GRAPH EXTRACTION SYSTEM (WITH ERROR RECOVERY)\n",
      "======================================================================\n",
      "\n",
      "[1] Initializing LLM...\n",
      "✓ LLM initialized\n",
      "\n",
      "[2] Checking for existing knowledge graph...\n",
      "✓ Starting fresh knowledge graph\n",
      "\n",
      "\n",
      "[3.1] Processing file 1/6\n",
      "\n",
      "======================================================================\n",
      "Processing file: data/SGK/1.txt\n",
      "======================================================================\n",
      "✓ Loaded 19369 characters\n",
      "✓ Split into 18 chunks\n",
      "\n",
      "Chunk 1/18: ] ✓ 12E/16T (R:1.3)\n",
      "Chunk 2/18: ] ✓ 15E/28T (R:1.9)\n",
      "Chunk 3/18: ] ✓ 19E/19T (R:1.0)\n",
      "Chunk 4/18: ] ✓ 14E/18T (R:1.3)\n",
      "Chunk 5/18: ] ✓ 14E/18T (R:1.3)\n",
      "Chunk 6/18: ] ✓ 18E/18T (R:1.0)\n",
      "Chunk 7/18: ] ✓ 15E/26T (R:1.7)\n",
      "Chunk 8/18: ] ✓ 34E/34T (R:1.0)\n",
      "Chunk 9/18: ] ✓ 12E/13T (R:1.1)\n",
      "Chunk 10/18: ] ✓ 20E/21T (R:1.1)\n",
      "Chunk 11/18: ] ✓ 15E/23T (R:1.5)\n",
      "Chunk 12/18: ] ✓ 15E/21T (R:1.4)\n",
      "Chunk 13/18: ] ✓ 14E/14T (R:1.0)\n",
      "Chunk 14/18: ] ✓ 19E/18T (R:0.9)\n",
      "Chunk 15/18: ] ✓ 17E/17T (R:1.0)\n",
      "Chunk 16/18: ] ✓ 22E/21T (R:1.0)\n",
      "Chunk 17/18: ] ✓ 12E/31T (R:2.6)\n",
      "Chunk 18/18: ] ✓ 20E/18T (R:0.9)\n",
      "\n",
      "→ Merging 18 successful chunks with existing KG...\n",
      "✓ Progress saved: 205 entities, 369 triplets\n",
      "\n",
      "[3.2] Processing file 2/6\n",
      "\n",
      "======================================================================\n",
      "Processing file: data/SGK/2.txt\n",
      "======================================================================\n",
      "✓ Loaded 12035 characters\n",
      "✓ Split into 12 chunks\n",
      "\n",
      "Chunk 1/12: ] ✓ 12E/20T (R:1.7)\n",
      "Chunk 2/12: ] ✓ 12E/12T (R:1.0)\n",
      "Chunk 3/12: ] ✓ 18E/16T (R:0.9)\n",
      "Chunk 4/12: ] ✓ 16E/17T (R:1.1)\n",
      "Chunk 5/12: ] ✓ 26E/29T (R:1.1)\n",
      "Chunk 6/12: ] ✓ 13E/15T (R:1.2)\n",
      "Chunk 7/12: ] ✓ 17E/20T (R:1.2)\n",
      "Chunk 8/12: ] ✓ 12E/11T (R:0.9)\n",
      "Chunk 9/12: ] ✓ 13E/13T (R:1.0)\n",
      "Chunk 10/12: ] ✓ 18E/16T (R:0.9)\n",
      "Chunk 11/12: ] ✓ 22E/23T (R:1.0)\n",
      "Chunk 12/12: ] ✓ 21E/19T (R:0.9)\n",
      "\n",
      "→ Merging 12 successful chunks with existing KG...\n",
      "✓ Progress saved: 330 entities, 576 triplets\n",
      "\n",
      "[3.3] Processing file 3/6\n",
      "\n",
      "======================================================================\n",
      "Processing file: data/SGK/3.txt\n",
      "======================================================================\n",
      "✓ Loaded 47500 characters\n",
      "✓ Split into 47 chunks\n",
      "\n",
      "Chunk 1/47: ] ✓ 24E/23T (R:1.0)\n",
      "Chunk 2/47: ] ✓ 12E/18T (R:1.5)\n",
      "Chunk 3/47: ] ✓ 29E/30T (R:1.0)\n",
      "Chunk 4/47: ] ✓ 16E/13T (R:0.8)\n",
      "Chunk 5/47: ] ✓ 18E/20T (R:1.1)\n",
      "Chunk 6/47: ] ✓ 13E/17T (R:1.3)\n",
      "Chunk 7/47: ] ✓ 15E/17T (R:1.1)\n",
      "Chunk 8/47: ] ✓ 16E/12T (R:0.8)\n",
      "Chunk 9/47: ] ✓ 23E/27T (R:1.2)\n",
      "Chunk 10/47: ] ✓ 18E/25T (R:1.4)\n",
      "Chunk 11/47: ] ✓ 16E/15T (R:0.9)\n",
      "Chunk 12/47: ] ✓ 23E/26T (R:1.1)\n",
      "Chunk 13/47: ] ✓ 19E/17T (R:0.9)\n",
      "Chunk 14/47: ] ✓ 20E/20T (R:1.0)\n",
      "Chunk 15/47: ] ✓ 13E/14T (R:1.1)\n",
      "Chunk 16/47: ] ✓ 16E/17T (R:1.1)\n",
      "Chunk 17/47: ] ✓ 22E/16T (R:0.7)\n",
      "Chunk 18/47: ] ✓ 24E/17T (R:0.7)\n",
      "Chunk 19/47: ] ✓ 19E/20T (R:1.1)\n",
      "Chunk 20/47: ] ✓ 21E/23T (R:1.1)\n",
      "Chunk 21/47: ] ✓ 19E/21T (R:1.1)\n",
      "Chunk 22/47: ] ✓ 13E/16T (R:1.2)\n",
      "Chunk 23/47: ] ✓ 15E/18T (R:1.2)\n",
      "Chunk 24/47: ] ✓ 19E/18T (R:0.9)\n",
      "Chunk 25/47: ] ✓ 12E/18T (R:1.5)\n",
      "Chunk 26/47: ] ✓ 12E/28T (R:2.3)\n",
      "Chunk 27/47: ] ✓ 16E/27T (R:1.7)\n",
      "Chunk 28/47: ] ✓ 17E/20T (R:1.2)\n",
      "Chunk 29/47: ] ✓ 14E/20T (R:1.4)\n",
      "Chunk 30/47: ] ✓ 18E/19T (R:1.1)\n",
      "Chunk 31/47: ] ✓ 16E/16T (R:1.0)\n",
      "Chunk 32/47: ] ✓ 29E/22T (R:0.8)\n",
      "Chunk 33/47: ] ✓ 17E/19T (R:1.1)\n",
      "Chunk 34/47: ] ✓ 21E/18T (R:0.9)\n",
      "Chunk 35/47: ] ✓ 11E/17T (R:1.5)\n",
      "Chunk 36/47: ] ✓ 16E/14T (R:0.9)\n",
      "Chunk 37/47: ] ✓ 21E/18T (R:0.9)\n",
      "Chunk 38/47: ] ✓ 18E/21T (R:1.2)\n",
      "Chunk 39/47: ] ✓ 18E/19T (R:1.1)\n",
      "Chunk 40/47: ] ✓ 17E/19T (R:1.1)\n",
      "Chunk 41/47: ] ✓ 18E/15T (R:0.8)\n",
      "Chunk 42/47: ] ✓ 18E/19T (R:1.1)\n",
      "Chunk 43/47: ] ✓ 19E/22T (R:1.2)\n",
      "Chunk 44/47: ] ✓ 16E/15T (R:0.9)\n",
      "Chunk 45/47: ] ✓ 16E/16T (R:1.0)\n",
      "Chunk 46/47: ] ✓ 21E/21T (R:1.0)\n",
      "Chunk 47/47: ] ✓ 8E/8T (R:1.0)\n",
      "\n",
      "→ Merging 47 successful chunks with existing KG...\n",
      "✓ Progress saved: 939 entities, 1459 triplets\n",
      "\n",
      "[3.4] Processing file 4/6\n",
      "\n",
      "======================================================================\n",
      "Processing file: data/SGK/4.txt\n",
      "======================================================================\n",
      "✓ Loaded 15443 characters\n",
      "✓ Split into 15 chunks\n",
      "\n",
      "Chunk 1/15: ] ✓ 17E/17T (R:1.0)\n",
      "Chunk 2/15: ] ✓ 25E/26T (R:1.0)\n",
      "Chunk 3/15: ] ✓ 13E/15T (R:1.2)\n",
      "Chunk 4/15: ] ✓ 20E/23T (R:1.1)\n",
      "Chunk 5/15: ] ✓ 10E/12T (R:1.2)\n",
      "Chunk 6/15: ] ✓ 22E/17T (R:0.8)\n",
      "Chunk 7/15: ] ✓ 12E/12T (R:1.0)\n",
      "Chunk 8/15: ] ✓ 20E/18T (R:0.9)\n",
      "Chunk 9/15: ] ✓ 18E/18T (R:1.0)\n",
      "Chunk 10/15: ] ✓ 27E/26T (R:1.0)\n",
      "Chunk 11/15: ] ✓ 17E/20T (R:1.2)\n",
      "Chunk 12/15: ] ✓ 17E/23T (R:1.4)\n",
      "Chunk 13/15: ] ✓ 25E/24T (R:1.0)\n",
      "Chunk 14/15: ] ✓ 10E/18T (R:1.8)\n",
      "Chunk 15/15: ] ✓ 11E/16T (R:1.5)\n",
      "\n",
      "→ Merging 15 successful chunks with existing KG...\n",
      "✓ Progress saved: 1129 entities, 1739 triplets\n",
      "\n",
      "[3.5] Processing file 5/6\n",
      "\n",
      "======================================================================\n",
      "Processing file: data/SGK/5.txt\n",
      "======================================================================\n",
      "✓ Loaded 17486 characters\n",
      "✓ Split into 17 chunks\n",
      "\n",
      "Chunk 1/17: ] ✓ 19E/23T (R:1.2)\n",
      "Chunk 2/17: ] ✓ 21E/23T (R:1.1)\n",
      "Chunk 3/17: ] ✓ 22E/26T (R:1.2)\n",
      "Chunk 4/17: ] ✓ 16E/21T (R:1.3)\n",
      "Chunk 5/17: ] ✓ 19E/32T (R:1.7)\n",
      "Chunk 6/17: ] ✓ 14E/15T (R:1.1)\n",
      "Chunk 7/17: ] ✓ 22E/21T (R:1.0)\n",
      "Chunk 8/17: ] ✓ 24E/27T (R:1.1)\n",
      "Chunk 9/17: ] ✓ 15E/20T (R:1.3)\n",
      "Chunk 10/17: ] ✓ 20E/19T (R:0.9)\n",
      "Chunk 11/17: ] ✓ 11E/13T (R:1.2)\n",
      "Chunk 12/17: ] ✓ 21E/32T (R:1.5)\n",
      "Chunk 13/17: ] ✓ 16E/15T (R:0.9)\n",
      "Chunk 14/17: ] ✓ 13E/18T (R:1.4)\n",
      "Chunk 15/17: ] ✓ 20E/27T (R:1.4)\n",
      "Chunk 16/17: ] ✓ 24E/26T (R:1.1)\n",
      "Chunk 17/17: ] ✓ 21E/20T (R:1.0)\n",
      "\n",
      "→ Merging 17 successful chunks with existing KG...\n",
      "✓ Progress saved: 1331 entities, 2115 triplets\n",
      "\n",
      "[3.6] Processing file 6/6\n",
      "\n",
      "======================================================================\n",
      "Processing file: data/SGK/6.txt\n",
      "======================================================================\n",
      "✓ Loaded 31276 characters\n",
      "✓ Split into 32 chunks\n",
      "\n",
      "Chunk 1/32: ] ✓ 10E/10T (R:1.0)\n",
      "Chunk 2/32: ] ✓ 26E/30T (R:1.2)\n",
      "Chunk 3/32: ] ✓ 19E/17T (R:0.9)\n",
      "Chunk 4/32: ] ✓ 16E/20T (R:1.2)\n",
      "Chunk 5/32: ] ✓ 15E/14T (R:0.9)\n",
      "Chunk 6/32: ] ✓ 17E/18T (R:1.1)\n",
      "Chunk 7/32: ] ✓ 19E/23T (R:1.2)\n",
      "Chunk 8/32: ] ✓ 13E/15T (R:1.2)\n",
      "Chunk 9/32: ] ✓ 20E/20T (R:1.0)\n",
      "Chunk 10/32: ] ✓ 18E/16T (R:0.9)\n",
      "Chunk 11/32: ] ✓ 17E/15T (R:0.9)\n",
      "Chunk 12/32: ] ✓ 17E/18T (R:1.1)\n",
      "Chunk 13/32: ] ✓ 12E/11T (R:0.9)\n",
      "Chunk 14/32: ] ✓ 12E/20T (R:1.7)\n",
      "Chunk 15/32: ] ✓ 12E/18T (R:1.5)\n",
      "Chunk 16/32: ] ✓ 19E/18T (R:0.9)\n",
      "Chunk 17/32: ] ✓ 10E/13T (R:1.3)\n",
      "Chunk 18/32: ] ✓ 15E/17T (R:1.1)\n",
      "Chunk 19/32: ] ✓ 16E/13T (R:0.8)\n",
      "Chunk 20/32: ] ✓ 15E/19T (R:1.3)\n",
      "Chunk 21/32: ] ✓ 17E/17T (R:1.0)\n",
      "Chunk 22/32: ] ✓ 28E/32T (R:1.1)\n",
      "Chunk 23/32: ] ✓ 25E/22T (R:0.9)\n",
      "Chunk 24/32: ] ✓ 16E/17T (R:1.1)\n",
      "Chunk 25/32: ] ✓ 19E/19T (R:1.0)\n",
      "Chunk 26/32: ] ✓ 14E/14T (R:1.0)\n",
      "Chunk 27/32: ] ✓ 20E/19T (R:0.9)\n",
      "Chunk 28/32: ] ✓ 11E/10T (R:0.9)\n",
      "Chunk 29/32: ] ✓ 14E/17T (R:1.2)\n",
      "Chunk 30/32: ] ✓ 14E/18T (R:1.3)\n",
      "Chunk 31/32: ] ✓ 20E/16T (R:0.8)\n",
      "Chunk 32/32: ] ✓ 11E/16T (R:1.5)\n",
      "\n",
      "→ Merging 32 successful chunks with existing KG...\n",
      "✓ Progress saved: 1644 entities, 2662 triplets\n",
      "\n",
      "======================================================================\n",
      "FINAL STATISTICS\n",
      "======================================================================\n",
      "Total Entities: 1644\n",
      "Total Relationships: 2662\n",
      "Relationship/Entity Ratio: 1.62\n",
      "\n",
      "Entity Type Distribution:\n",
      "  Khái niệm: 666\n",
      "  Địa điểm: 217\n",
      "  Tổ chức: 211\n",
      "  Sự kiện: 194\n",
      "  Khác: 135\n",
      "  Quốc gia: 88\n",
      "  Thời gian: 55\n",
      "  Hiệp ước: 45\n",
      "  Nhân vật: 33\n",
      "\n",
      "[4] Saving final outputs...\n",
      "✓ JSON saved: graph_documents_v3.json\n",
      "✓ TTL saved: graph_documents_v3.ttl\n",
      "\n",
      "======================================================================\n",
      "EXTRACTION COMPLETE!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    INPUT_FILES = [\n",
    "        r\"data/SGK/1.txt\",\n",
    "        r\"data/SGK/2.txt\",\n",
    "        r\"data/SGK/3.txt\",\n",
    "        r\"data/SGK/4.txt\",\n",
    "        r\"data/SGK/5.txt\",\n",
    "        r\"data/SGK/6.txt\",\n",
    "    ]\n",
    "    \n",
    "    OUTPUT_JSON = \"graph_documents_v3.json\"\n",
    "    OUTPUT_TTL = \"graph_documents_v3.ttl\"\n",
    "    \n",
    "    create_knowledge_graph_incremental(\n",
    "        INPUT_FILES,\n",
    "        OUTPUT_JSON,\n",
    "        OUTPUT_TTL,\n",
    "        delay_between_chunks=2.0,\n",
    "        save_after_each_file=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
